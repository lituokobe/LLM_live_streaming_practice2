[{"bbox": [112, 102, 1578, 248], "category": "Text", "text": "目前Flink RedisConnector仅支持at-least-once语义,我们可以借助Redis数据存储特性可以实现exactly-once语义,例如:利用Redis的Hash结构key不能重复的特性来实现exactly-once语义,将Flink处理的数据流写入到Redis中,在编写代码之前需要在项目中导入如下依赖:"}, {"bbox": [129, 301, 857, 510], "category": "Text", "text": "```xml\n<dependency>\n  <groupId>org.apache.bahir</groupId>\n  < artifactId>flink-connector-redis_2.12 </ artifactId>\n  <version>1.1.0</ version>\n</dependency>\n```"}, {"bbox": [112, 560, 942, 606], "category": "Section-header", "text": "## 案例:读取Socket数据统计WordCount实时写入Redis。"}, {"bbox": [135, 644, 387, 688], "category": "List-item", "text": "* **Java代码实现**"}, {"bbox": [126, 742, 1602, 2097], "category": "Text", "text": "```java\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n/*\n * Socket中输入数据如下:\n * hello,flink\n * hello,spark\n * hello,hadoop\n * hello,java\n */\nuloadSource <String> ds1 = env.socketTextStream(\"node5\", 9999);\n\n//统计wordcount\nSingleOutputStreamOperator<Tuple2<String, Integer>> result = ds1.map((FlatMapFunction <String, String>)\n    String[] arr = s.split(\",\");\n    for (String word : arr) {\n        collector.collect(word);\n    }\n).returns(Types.STRING)\n.map(one -> Tuple2.of(one, 1)).returns(Types.TUPLE(Types.STRING, Types.INT))\n.keyBy.tp -> tp.f0)\n.sum(1);\n\n//准备RedisSink对象\nFlinkJedisPoolConfig conf = new FlinkJedisPoolConfig.Builder()\n    .setHost(\"node4\")\n    .setPort(6379)\n    .setDatabase(1)\n    .build();\n\nRedisSink<Tuple2<String, Integer>> redisSink = new RedisSink <>(conf, new RedisMapper<Tuple2<String, I\n    @Override\n    public RedisCommandDescription getCommandDescription()\n        //指定Redis命令描述,不需预先创建Redis表\n```"}]