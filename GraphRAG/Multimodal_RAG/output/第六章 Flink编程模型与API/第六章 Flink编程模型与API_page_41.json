[{"bbox": [119, 99, 1605, 1210], "category": "Text", "text": "```kotlin\n* 004,188,186,busy,4000,40\n* 005,188,187,busy,5000,50\n*/\nval ds: DataStream(String] = env.socketTextStream(\"node5\", 9999)\n\n//设置FlinkSink\nval fileSink: FileSink(String] = FileSink.forRowFormat(new Path(\"./out/scala-file-out\"), new SimpleStringEncoder{}\n//设置桶目录检查间隔，默认1分钟\n.withBucketCheckInterval(1000 * 60)\n//设置滚动策略\n.withRollingPolicy(\n    DefaultRollingPolicy builder()\n    //桶不活跃的间隔时长, 默认1分钟\n    .withInactivityInterval Durations.ofSeconds(30))\n    //设置文件多大后生成新的文件, 默认128M\n    .withMaxPartSize(MemorySize.ofMebiBytes(1024))\n    //设置每隔多长时间生成新的文件, 默认1分钟\n    .withRolloverInterval(Durations.ofSeconds(10))\n    .build()\n)\n.build()\n\n//写出到文件\nds.sinkTo(fileSink)\n\nenv.execute()\n```"}, {"bbox": [110, 1268, 428, 1328], "category": "Section-header", "text": "## 6.6.2 JdbcSink"}, {"bbox": [110, 1360, 1554, 1509], "category": "Text", "text": "Flink的JdbcSink是用于将数据写入关系型数据库的输出组件, 它支持灵活的配置和可靠的事务处理, 包括批量写入和并行写入功能。用户可以自定义数据转换逻辑, 并通过提供数据库连接信息和SQL语句来指定目标表和插入操作。"}, {"bbox": [110, 1544, 1554, 1692], "category": "Text", "text": "JdbcSink提供了高性能和可靠的方式, 将流处理作业的结果或数据持久化到数据库中, 它支持at-least-once和exactly-once语义, 确保数据被准确写入数据库一次, 避免重复写入或数据丢失的问题。"}, {"bbox": [110, 1743, 608, 1799], "category": "Section-header", "text": "### 6.6.2.1 at-least-once语义"}, {"bbox": [110, 1831, 1554, 1930], "category": "Text", "text": "Flink数据写出到JdbcSink提供了at-least-once写出语义, 如果业务允许可以通过编写upsert 更新 SQL 可以实现精准一次的写出语义。"}, {"bbox": [110, 1964, 687, 2014], "category": "Text", "text": "在使用Flink JdbcSink时使用格式如下:"}]