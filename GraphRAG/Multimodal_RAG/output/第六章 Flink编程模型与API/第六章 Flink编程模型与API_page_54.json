[{"bbox": [123, 99, 1031, 736], "category": "Text", "text": "```scala\n* hello,world\n* hello,flink\n* hello,scala\n* hello,spark\n* hello,hadoop\n}\n\nval ds: DataStream String = env.socketTextStream(\"node5\", 9999)\n//统计wordCount\nval result: DataStream String = ds flatMap _.split(\",\")\n.map((_, 1))\n.keyBy(_._1)\n.sum(1)\n.map(t => {\n  t._1 + \"-\" + t._2\n})\n```"}, {"bbox": [123, 778, 1455, 1290], "category": "Text", "text": "//准备KafkaSink对象\nval kafkaSink: KafkaSink String = KafkaSink builder()\n.setBootstrapServers(\"node1:9092, node2:9092, node3:9092\")\n//设置事务超时时间,最大不超过kafka broker的事务最大超时时间限制: max.transaction.timeout.ms\n PROPERTY(\"transaction.timeout.ms\", 15 * 60 * 1000L + \"\")\n.setRecord(KafkaRecord serialization Schema builder()\n.setTopic(\"flink-topic\")\ninzideValue serialization Schema(new SimpleStringSchema())\n.build()\n)\n.setDeliveryGuarantee(DeliveryGuarantee EXACTLYONCE)\n.build()\n```"}, {"bbox": [123, 1332, 450, 1461], "category": "Text", "text": "//将结果写入Kafka\nresult sinkTo(kafkaSink)\nenv.execute()"}, {"bbox": [135, 1509, 688, 1559], "category": "Title", "text": "## Scala代码实现-写出有key和value"}, {"bbox": [123, 1603, 1472, 2072], "category": "Text", "text": "```scala\nobject KafkaSink With Key Value Test {\n  def main(args: Array String): Unit = {\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n    import org.apache.flinkKAstreaming api Play\n    val result: DataStream [(String, Int)] = env.socketTextStream(\"node5\", 9999)\n      flatMap_.split(\",\")\n      .map((_, 1))\n      .keyBy(_._1)\n      .sum(1)\n\n  //准备Kafka Sink 对象\n}\n```"}]