[{"bbox": [125, 101, 1603, 1671], "category": "Text", "text": "```groovy\n* Socket中输入数据如下:\n* hello,flink\n* hello,spark\n* hello,hadoop\n* hello,java\n*/\nuloStreamSource <String> ds1 = env.socketTextStream(\"node5\", 9999);\n\n//统计wordcount\nSingleOutputStreamOperator <String> result = ds1.map((FlatMapFunction <String, String>) (s, collector) -\n    String[] arr = s.split(\",\");\n    for (String word : arr) {\n        collector.collect(word);\n    }\n).returns(Types.STRING)\n.map(one -> Tuple2.of(one, 1)).returns(Types.TUPLE(Types.STRING, Types.INT))\n.keyBy.tp -> tp.f0)\n.sum(1)\n.map(one -> one.f0 + \"-\" + one.f1).returns(Types.STRING);\n\n//准备Flink KafkaSink对象\nKafkaSink <String> kafkaSink = KafkaSink . <String> builder()\n    .setBootstrapServers(\"node1:9092, node2:9092, node3:9092\")\n    //设置事务超时时间,最大不超过kafka broker的事务最大超时时间限制: max.transaction.timeout.ms\n    .setProperty(\"transaction.timeout.ms\", 15 * 60 * 1000L + \"\")\n    .setRecord(KafkaRecord serializationSchema . builder()\n        .setTopic(\"flink-topic\")\n        . sou\n        .build()\n    )\n    .setDeliveryGuarantee(DeliveryGuarantee .EXACTLY_ONCE)\n    .build();\n\n//将结果写出到Kafka\nresult.sinkTo(kafkaSink);\n\nenv.execute();\n```"}, {"bbox": [135, 1723, 537, 1770], "category": "Title", "text": "## Scala代码实现-只有key"}, {"bbox": [125, 1821, 1437, 2069], "category": "Text", "text": "```groovy\nval env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n//设置隐式转换\nimport org.apache flink api Play\n/*\n * Socket 中输入数据格式:\n```"}]