[{"bbox": [123, 102, 827, 530], "category": "Text", "text": "```groovy\n)).map(one -> {\n    val split = one.split(\",\")\n    new Student split(0).etoInt, split(1), split(2).etoInt\n}).filter _.id > 1\n.print()\n\nenv.execute()\n}\n}\n```"}, {"bbox": [114, 590, 507, 652], "category": "Section-header", "text": "## 6.3 Flink Source"}, {"bbox": [110, 689, 1563, 836], "category": "Text", "text": "Sources模块定义了Stream API中数据输入操作,Flink中内置了很多数据源Source,例如:文件数据源、Socket数据源、集合数据源,同时也支持第三方数据源,例如:Kafka数据源、自定义数据源,下面分别使用Stream API进行演示。"}, {"bbox": [114, 889, 475, 943], "category": "Section-header", "text": "### 6.3.1 File Source"}, {"bbox": [110, 980, 1582, 1124], "category": "Text", "text": "在Flink早先版本读取文本数据我们可以使用`env.readTextFile`方法来实现,在后续版本中该方法被标记过时,建议使用Stream Connectors来完成。这里以读取HDFS中的文件进行演示。"}, {"bbox": [114, 1164, 756, 1208], "category": "Text", "text": "在linux中创建`data.txt`文件,写入如下内容:"}, {"bbox": [126, 1262, 224, 1383], "category": "Text", "text": "```\nhello,a\nhello,b\nhello,c\n```"}, {"bbox": [114, 1438, 745, 1480], "category": "Text", "text": "将以上文件上传至HDFS `/flinkdata`目录中:"}, {"bbox": [126, 1534, 596, 1700], "category": "Text", "text": "```sh\n#创建HDFS 目录\nhdfs dfs -mkdir /flinkdata\n#上传数据\nhdfs dfs -put ./data.txt /flinkdata/\n```"}, {"bbox": [114, 1752, 1543, 1796], "category": "Text", "text": "编写Flink读取文件Source代码前,无论是Java API还是Scala API都需要在项目中导入如下依赖:"}, {"bbox": [126, 1849, 758, 2057], "category": "Text", "text": "```xml\n <!-- DataStream files connector -->\n<dependency>\n<groupId>org.apache.flink</groupId>\n<artifactId>flink-connector-files</artifactId>\n<version>${flink.version}</version>\n```"}]