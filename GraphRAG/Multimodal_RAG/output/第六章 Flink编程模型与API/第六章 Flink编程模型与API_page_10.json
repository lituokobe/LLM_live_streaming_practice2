[{"bbox": [122, 99, 1441, 699], "category": "Text", "text": "```groovy\nval env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\nimport org.apache flink api scala._\n\nval arrays: Array[StationLog] = Array(StationLog(\"001\", \"186\", \"187\", \"busy\", 1000L, 0L),\nStationLog(\"002\", \"187\", \"186\", \"fail\", 2000L, 0L),\nStationLog(\"003\", \"186\", \"188\", \"busy\", 3000L, 0L),\nStationLog(\"004\", \"188\", \"186\", \"busy\", 4000L, 0L),\nStationLog(\"005\", \"188\", \"187\", \"busy\", 5000L, 0L))\n\nval dataStream: DataStream[StationLog] = env.fromCollection arrays\ndataStream.print()\n\nenv.execute()\n```"}, {"bbox": [109, 743, 1562, 842], "category": "Text", "text": "除了以上可以从集合中获取ibiStream之外，类似的还有从多个元素对象获取对应的ibiStream\n方法如下："}, {"bbox": [122, 888, 680, 935], "category": "Text", "text": "```\nenv.fromElements(elem1, elem2, elem3...)\n```"}, {"bbox": [109, 981, 575, 1031], "category": "Text", "text": "集合Source也常用于程序测试。"}, {"bbox": [109, 1078, 524, 1138], "category": "Title", "text": "### 6.3.4 Kafka Source"}, {"bbox": [109, 1172, 1576, 1370], "category": "Text", "text": "在实时处理场景中Flink读取kafka中的数据是最常见的场景, Flink在操作Kafka时天生支持容错、数据精准消费一次, 所以Flink与Kafka是一对“黄金搭档”, 关于两者整合容错机制原理在后续章节再介绍, 这里主要从API层面实现Flink操作Kafka数据, Flink读取Kafka中的数据需要配置Kafka Connector依赖, 依赖如下:"}, {"bbox": [125, 1418, 779, 1677], "category": "Text", "text": "```xml\n <!-- 读取Kafka 依赖 -->\n<dependency>\n    <groupId>org.apache flink</groupId>\n    <artifactId>flink-connector-kafka</ artifactId>\n    <version>${flink.version}</version>\n</dependency>\n```"}, {"bbox": [109, 1722, 1557, 1822], "category": "Text", "text": "读取Kafka中的数据时可以选择读取每条数据的key和value,也可以选择只读取Value,两者API写法不同。"}, {"bbox": [109, 1866, 692, 1927], "category": "Title", "text": "#### 6.3.4.1 读取Kafka中Value数据"}, {"bbox": [132, 1958, 389, 2007], "category": "Text", "text": "* Java代码如下"}]